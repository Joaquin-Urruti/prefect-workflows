# Prefect Workflows on Docker

A production-ready Prefect workflow orchestration platform running on Docker. This setup provides a complete, self-contained environment for building, deploying, and monitoring data pipelines with full data persistence.

[![Prefect](https://img.shields.io/badge/Prefect-3.6.9-blue)](https://www.prefect.io/)
[![Python](https://img.shields.io/badge/Python-3.13-green)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Deployment Guide](#deployment-guide)
- [Usage Guide](#usage-guide)
- [Configuration](#configuration)
- [Data Persistence](#data-persistence)
- [Monitoring](#monitoring)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Documentation](#documentation)
- [License](#license)

---

## Features

- ğŸš€ **Production-Ready Setup**: Complete Docker Compose orchestration with health checks
- ğŸ’¾ **Full Data Persistence**: PostgreSQL, Redis, logs, and outputs persisted on disk
- ğŸ”„ **Automatic Scheduling**: Cron-based and interval-based workflow scheduling
- ğŸ“Š **Built-in Monitoring**: Web UI for real-time monitoring and debugging
- ğŸ³ **Containerized**: Isolated environment with all dependencies included
- ğŸ”§ **Easy Configuration**: Environment variables and volume mappings
- ğŸ“ **Comprehensive Logging**: Centralized logs with automatic rotation
- ğŸ¯ **Process Worker**: Local worker pool for reliable task execution
- ğŸ”’ **Secure**: Environment-based secrets management

---

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Host Machine                            â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Docker Compose Stack                      â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚  PostgreSQL  â”‚  â”‚    Redis     â”‚  â”‚   Prefect   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   Database   â”‚  â”‚   Message    â”‚  â”‚   Server    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚    Broker    â”‚  â”‚  (API:4200) â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Port: 5432   â”‚  â”‚ Port: 6379   â”‚  â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚         â”‚                  â”‚                 â”‚         â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚  â”‚                            â”‚                            â”‚ â”‚
â”‚  â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚                     â”‚    Prefect    â”‚                  â”‚ â”‚
â”‚  â”‚                     â”‚   Services    â”‚                  â”‚ â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚ â”‚
â”‚  â”‚                            â”‚                            â”‚ â”‚
â”‚  â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚                     â”‚    Prefect    â”‚                  â”‚ â”‚
â”‚  â”‚                     â”‚    Worker     â”‚                  â”‚ â”‚
â”‚  â”‚                     â”‚ (Executes     â”‚                  â”‚ â”‚
â”‚  â”‚                     â”‚  Workflows)   â”‚                  â”‚ â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  Persistent Storage (Host Directories):                    â”‚
â”‚  ./data/postgres/  ./data/redis/  ./logs/  ./outputs/     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

| Component | Description | Port | Health Check |
|-----------|-------------|------|--------------|
| **PostgreSQL** | Stores Prefect metadata, flow runs, task states | 5432 | âœ“ |
| **Redis** | Message broker for real-time communication | 6379 | âœ“ |
| **Prefect Server** | REST API and web UI | 4200 | âœ“ |
| **Prefect Services** | Background services (scheduler, etc.) | - | - |
| **Prefect Worker** | Executes workflows from work pool | - | âœ“ |

---

## Project Structure

```
prefect-workflows/
â”œâ”€â”€ .github/                    # GitHub workflows (if using CI/CD)
â”œâ”€â”€ .history/                   # Local history (gitignored)
â”œâ”€â”€ data/                       # Persistent data (gitignored)
â”‚   â”œâ”€â”€ postgres/              # PostgreSQL database files
â”‚   â””â”€â”€ redis/                 # Redis persistence files
â”œâ”€â”€ logs/                       # Application logs (gitignored)
â”‚   â”œâ”€â”€ server/                # Prefect server logs
â”‚   â”œâ”€â”€ services/              # Prefect services logs
â”‚   â””â”€â”€ worker/                # Worker execution logs
â”œâ”€â”€ outputs/                    # Workflow outputs (gitignored)
â”‚   â””â”€â”€ [workflow_outputs]/    # Task outputs, reports, datasets
â”œâ”€â”€ scripts/                    # Workflow definitions
â”‚   â”œâ”€â”€ __pycache__/           # Python cache (gitignored)
â”‚   â”œâ”€â”€ main.py                # Example simple script
â”‚   â”œâ”€â”€ test.py                # Example deployment script
â”‚   â””â”€â”€ [your_workflows].py    # Your custom workflows
â”œâ”€â”€ .env                        # Environment variables (gitignored)
â”œâ”€â”€ .gitignore                 # Git ignore patterns
â”œâ”€â”€ deploy.md                  # Deployment guide
â”œâ”€â”€ docker-compose.yml         # Docker Compose configuration
â”œâ”€â”€ Dockerfile                 # Worker container definition
â”œâ”€â”€ prefect-user-guide-en.md   # User guide (English)
â”œâ”€â”€ prefect-user-guide-sp.md   # User guide (Spanish)
â”œâ”€â”€ pyproject.toml             # Python dependencies
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ uv.lock                    # Locked dependencies

Key Directories:
â”œâ”€â”€ scripts/        â†’ Place your Python workflows here
â”œâ”€â”€ outputs/        â†’ Workflow outputs are saved here
â”œâ”€â”€ logs/           â†’ All logs are persisted here
â””â”€â”€ data/           â†’ Database persistence
```

### File Descriptions

#### Configuration Files

- **`docker-compose.yml`**: Orchestrates all services (PostgreSQL, Redis, Prefect)
- **`Dockerfile`**: Defines the worker container with Python 3.13 and dependencies
- **`pyproject.toml`**: Python project configuration and dependencies
- **`uv.lock`**: Locked dependency versions for reproducibility
- **`.env`**: Environment variables (create from template below)
- **`.gitignore`**: Prevents sensitive and generated files from being committed

#### Documentation

- **`README.md`**: This file - main project documentation
- **`deploy.md`**: Step-by-step deployment instructions
- **`prefect-user-guide-en.md`**: Comprehensive user guide (English)
- **`prefect-user-guide-sp.md`**: Comprehensive user guide (Spanish)

#### Scripts

- **`scripts/`**: Your workflow definitions go here
  - `test.py`: Example workflow with scheduling
  - `main.py`: Simple example script

---

## Prerequisites

Before you begin, ensure you have the following installed:

### Required Software

- **Docker** (version 20.10 or higher)
  ```bash
  docker --version
  ```

- **Docker Compose** (version 2.0 or higher)
  ```bash
  docker compose version
  ```

### Optional

- **Git** (for version control)
- **Python 3.13+** (for local development)
- **uv** (for dependency management)

### System Requirements

- **RAM**: Minimum 8GB (16GB recommended)
- **Disk**: At least 10GB free space
- **CPU**: 2 cores minimum (4 cores recommended)
- **OS**: Linux, macOS, or Windows with WSL2

---

## Quick Start

Get up and running in 5 minutes:

### 1. Clone the Repository

```bash
git clone <repository-url>
cd prefect-workflows
```

### 2. Create Required Directories

```bash
mkdir -p outputs
```

### 3. Configure Environment (Optional)

Create a `.env` file for custom configuration:

```bash
cat > .env << 'EOF'
# PostgreSQL Configuration
PREFECT_POSTGRES_USER=prefect
PREFECT_POSTGRES_PASSWORD=your_secure_password_here
PREFECT_POSTGRES_DB=prefect

# Timezone
TZ=America/Argentina/Buenos_Aires
EOF
```

### 4. Build and Start Services

```bash
# Build the worker image
docker compose build --no-cache prefect-worker

# Start all services
docker compose up -d
```

### 5. Create Work Pool

```bash
docker compose exec prefect-server prefect work-pool create local-pool --type process
```

### 6. Access the UI

Open your browser and navigate to:

```
http://localhost:4200
```

### 7. Deploy Your First Workflow

```bash
docker compose exec prefect-worker python scripts/test.py
```

**That's it!** Your Prefect environment is ready. ğŸ‰

---

## Deployment Guide

### Initial Setup

#### Step 1: Environment Preparation

1. **Create necessary directories:**
   ```bash
   mkdir -p outputs
   ```

2. **Configure environment variables** (optional):
   ```bash
   cp .env.example .env  # If you have an example file
   # OR create manually
   nano .env
   ```

   Add the following:
   ```env
   PREFECT_POSTGRES_USER=prefect
   PREFECT_POSTGRES_PASSWORD=secure_password_123
   PREFECT_POSTGRES_DB=prefect
   TZ=America/Argentina/Buenos_Aires
   ```

#### Step 2: Build Docker Images

```bash
docker compose build --no-cache prefect-worker
```

This will:
- Download Prefect 3.6.9 base image with Python 3.13
- Install `uv` dependency manager
- Install all dependencies from `pyproject.toml`
- Copy your workflow scripts
- Set up directories and permissions

#### Step 3: Start Services

```bash
docker compose up -d
```

Verify all services are running:

```bash
docker compose ps
```

Expected output:
```
NAME               STATUS                PORTS
prefect-postgres   Up (healthy)         5432/tcp
prefect-redis      Up (healthy)         6379/tcp
prefect-server     Up (healthy)         0.0.0.0:4200->4200/tcp
prefect-services   Up
prefect-worker     Up (healthy)
```

#### Step 4: Initialize Prefect

Create the work pool:

```bash
docker compose exec prefect-server prefect work-pool create local-pool --type process
```

Verify creation:

```bash
docker compose exec prefect-server prefect work-pool ls
```

#### Step 5: Deploy Workflows

Deploy the example workflow:

```bash
docker compose exec prefect-worker python scripts/test.py
```

Verify in the UI:
1. Open http://localhost:4200
2. Navigate to "Deployments"
3. You should see "hello-world/hello-world"

### Updating and Maintenance

#### Update Scripts

Scripts are mounted as volumes, so changes are reflected immediately:

```bash
# Edit your script
nano scripts/my_workflow.py

# No rebuild needed - just redeploy
docker compose exec prefect-worker python scripts/my_workflow.py
```

#### Update Dependencies

If you modify `pyproject.toml`:

```bash
# Rebuild the worker
docker compose build prefect-worker

# Restart the worker
docker compose up -d prefect-worker
```

#### Update Prefect Version

1. Edit `docker-compose.yml` and `Dockerfile`
2. Change image version: `prefecthq/prefect:3-python3.13` â†’ `prefecthq/prefect:X-python3.13`
3. Rebuild and restart:
   ```bash
   docker compose build --no-cache
   docker compose down
   docker compose up -d
   ```

---

## Usage Guide

### Creating Workflows

#### Basic Workflow Structure

Create a new file in `scripts/my_workflow.py`:

```python
from prefect import flow, task
from prefect.logging import get_run_logger
from datetime import datetime

@task
def extract_data(source: str):
    """Extract data from source"""
    logger = get_run_logger()
    logger.info(f"Extracting data from {source}")
    # Your extraction logic
    return {"data": "sample"}

@task
def transform_data(data: dict):
    """Transform the data"""
    logger = get_run_logger()
    logger.info("Transforming data")
    # Your transformation logic
    return data

@task
def load_data(data: dict, destination: str):
    """Load data to destination"""
    logger = get_run_logger()
    logger.info(f"Loading data to {destination}")
    # Your loading logic
    return True

@flow(name="My ETL Pipeline")
def my_etl_pipeline(source: str, destination: str):
    """Complete ETL pipeline"""
    logger = get_run_logger()
    logger.info("Starting ETL pipeline")

    raw_data = extract_data(source)
    clean_data = transform_data(raw_data)
    success = load_data(clean_data, destination)

    logger.info("Pipeline completed")
    return success

if __name__ == "__main__":
    # Deploy with schedule
    my_etl_pipeline.deploy(
        name="daily-etl",
        work_pool_name="local-pool",
        cron="0 2 * * *",  # 2 AM daily
        parameters={
            "source": "api.example.com",
            "destination": "/app/outputs/data.json"
        },
        tags=["etl", "production"]
    )
```

#### Running Workflows

**Test locally:**
```bash
docker compose exec prefect-worker python scripts/my_workflow.py
```

**Deploy with schedule:**
```bash
# The deployment happens when you run the script with deploy()
docker compose exec prefect-worker python scripts/my_workflow.py
```

**Run manually from CLI:**
```bash
docker compose exec prefect-server prefect deployment run "my-etl-pipeline/daily-etl"
```

**View runs:**
```bash
docker compose exec prefect-server prefect flow-run ls --limit 10
```

### Scheduling Options

#### Cron Schedule

```python
# Every day at 2 AM
my_flow.deploy(cron="0 2 * * *", ...)

# Every 15 minutes
my_flow.deploy(cron="*/15 * * * *", ...)

# Weekdays at 9 AM
my_flow.deploy(cron="0 9 * * 1-5", ...)
```

#### Interval Schedule

```python
from datetime import timedelta

my_flow.deploy(
    interval=timedelta(hours=2),  # Every 2 hours
    ...
)
```

#### Manual Only (No Schedule)

```python
my_flow.deploy(
    name="manual-workflow",
    work_pool_name="local-pool"
    # No cron or interval
)
```

### Working with Outputs

Save workflow outputs to the `outputs/` directory:

```python
from pathlib import Path
from datetime import datetime
import json

@task
def save_results(data: dict, filename: str):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filepath = Path(f"/app/outputs/{filename}_{timestamp}.json")

    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

    return str(filepath)
```

Access outputs from host:
```bash
ls -lh outputs/
cat outputs/results_*.json
```

### Logging Best Practices

```python
from prefect.logging import get_run_logger

@task
def my_task():
    logger = get_run_logger()

    logger.debug("Detailed debug information")
    logger.info("General information")
    logger.warning("Warning message")
    logger.error("Error occurred")
    logger.critical("Critical issue")
```

View logs:
```bash
# Real-time logs
docker compose logs -f prefect-worker

# Specific service
docker compose logs -f prefect-server

# From file system
tail -f logs/worker/*.log
```

---

## Configuration

### Environment Variables

Configure via `.env` file or `docker-compose.yml`:

#### PostgreSQL Settings

```env
PREFECT_POSTGRES_USER=prefect
PREFECT_POSTGRES_PASSWORD=your_password
PREFECT_POSTGRES_DB=prefect
```

#### Prefect Settings

```env
PREFECT_API_URL=http://prefect-server:4200/api
PREFECT_LOGGING_LEVEL=INFO
PYTHONUNBUFFERED=1
```

#### Timezone

```env
TZ=America/Argentina/Buenos_Aires
```

### Volume Mappings

Current volume mappings in `docker-compose.yml`:

```yaml
volumes:
  # PostgreSQL data
  - ./data/postgres:/var/lib/postgresql/data

  # Redis data
  - ./data/redis:/data

  # Scripts (mounted for easy editing)
  - ./scripts:/app/scripts

  # Outputs (task results)
  - ./outputs:/app/outputs

  # Logs
  - ./logs/server:/root/.prefect/logs
  - ./logs/worker:/root/.prefect/logs
```

### Resource Limits

Default resource limits (adjust in `docker-compose.yml`):

```yaml
deploy:
  resources:
    limits:
      memory: 5G  # Maximum memory per service
```

---

## Data Persistence

All important data is persisted on the host machine:

### Directory Structure

```
data/
â”œâ”€â”€ postgres/          # PostgreSQL database
â”‚   â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ global/
â”‚   â””â”€â”€ ...
â””â”€â”€ redis/             # Redis snapshots
    â””â”€â”€ dump.rdb

logs/
â”œâ”€â”€ server/            # Server logs
â”œâ”€â”€ services/          # Services logs
â””â”€â”€ worker/            # Worker logs

outputs/               # Workflow outputs
â””â”€â”€ [your_files]
```

### Backup Strategy

#### Create Backup

```bash
# Stop services
docker compose down

# Create timestamped backup
tar -czf prefect-backup-$(date +%Y%m%d_%H%M%S).tar.gz data/ logs/ outputs/

# Restart services
docker compose up -d
```

#### Restore from Backup

```bash
# Stop services
docker compose down

# Remove current data (CAUTION!)
rm -rf data/ logs/ outputs/

# Extract backup
tar -xzf prefect-backup-TIMESTAMP.tar.gz

# Restart services
docker compose up -d
```

#### Automated Backups

Create a backup script `scripts/backup.sh`:

```bash
#!/bin/bash
BACKUP_DIR="/path/to/backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/prefect-backup-$DATE.tar.gz"

# Create backup
tar -czf "$BACKUP_FILE" data/ logs/ outputs/

# Keep only last 7 backups
ls -t $BACKUP_DIR/prefect-backup-* | tail -n +8 | xargs rm -f

echo "Backup created: $BACKUP_FILE"
```

Add to crontab:
```bash
# Daily backup at 3 AM
0 3 * * * cd /path/to/prefect-workflows && ./scripts/backup.sh
```

---

## Monitoring

### Web UI

Access the Prefect UI at **http://localhost:4200**

Features:
- **Dashboard**: Overview of flow runs, deployments, work pools
- **Flows**: View all registered flows
- **Deployments**: Manage deployment schedules
- **Flow Runs**: Monitor execution history
- **Work Pools**: View worker status
- **Blocks**: Manage configuration blocks
- **Notifications**: Set up alerts

### Command-Line Monitoring

#### Check Service Status

```bash
docker compose ps
```

#### View Logs

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f prefect-worker

# Last 100 lines
docker compose logs --tail 100 prefect-server

# Since 1 hour ago
docker compose logs --since 1h prefect-worker
```

#### Resource Usage

```bash
# Real-time stats
docker stats

# Disk usage
docker system df -v
```

#### Prefect Status

```bash
# List flows
docker compose exec prefect-server prefect flow ls

# List deployments
docker compose exec prefect-server prefect deployment ls

# Recent flow runs
docker compose exec prefect-server prefect flow-run ls --limit 20

# Work pool status
docker compose exec prefect-server prefect work-pool ls
```

### Health Checks

All critical services have health checks:

```bash
# Check health status
docker compose ps

# Detailed health check
docker inspect prefect-server | jq '.[0].State.Health'
```

---

## Examples

### Example 1: Simple ETL Pipeline

```python
from prefect import flow, task
from prefect.logging import get_run_logger
import requests
import json
from datetime import datetime

@task(retries=3, retry_delay_seconds=60)
def extract_from_api(api_url: str):
    logger = get_run_logger()
    logger.info(f"Extracting from {api_url}")

    response = requests.get(api_url, timeout=30)
    response.raise_for_status()

    return response.json()

@task
def transform_records(data: list):
    logger = get_run_logger()
    logger.info(f"Transforming {len(data)} records")

    transformed = []
    for record in data:
        transformed.append({
            "id": record["id"],
            "name": record["name"].upper(),
            "processed_at": datetime.now().isoformat()
        })

    return transformed

@task
def load_to_file(data: list, filename: str):
    logger = get_run_logger()
    filepath = f"/app/outputs/{filename}"

    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

    logger.info(f"Loaded {len(data)} records to {filepath}")
    return filepath

@flow(name="API ETL Pipeline")
def api_etl_pipeline():
    # Extract
    raw_data = extract_from_api("https://jsonplaceholder.typicode.com/users")

    # Transform
    clean_data = transform_records(raw_data)

    # Load
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = load_to_file(clean_data, f"users_{timestamp}.json")

    return output_file

if __name__ == "__main__":
    api_etl_pipeline.deploy(
        name="daily-api-etl",
        work_pool_name="local-pool",
        cron="0 2 * * *",
        tags=["etl", "api"]
    )
```

### Example 2: Parallel Processing

```python
from prefect import flow, task
import time

@task
def process_batch(batch_id: int, items: list):
    time.sleep(2)  # Simulate processing
    return {
        "batch_id": batch_id,
        "items_processed": len(items),
        "status": "success"
    }

@flow
def parallel_batch_processing():
    batches = [
        [1, 2, 3, 4, 5],
        [6, 7, 8, 9, 10],
        [11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20]
    ]

    # Process batches in parallel
    futures = []
    for i, batch in enumerate(batches):
        future = process_batch.submit(i, batch)
        futures.append(future)

    # Wait for all to complete
    results = [f.result() for f in futures]

    return results
```

### Example 3: Error Handling

```python
from prefect import flow, task
from prefect.logging import get_run_logger

@task(retries=3, retry_delay_seconds=30)
def risky_operation(value: int):
    logger = get_run_logger()

    try:
        if value < 0:
            raise ValueError("Value must be positive")

        result = 100 / value
        logger.info(f"Success: {result}")
        return result

    except ValueError as e:
        logger.error(f"Validation error: {e}")
        raise
    except ZeroDivisionError as e:
        logger.error(f"Division error: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise

@flow
def error_handling_flow():
    results = []

    for value in [10, 5, 0, -1]:
        try:
            result = risky_operation(value)
            results.append(result)
        except Exception:
            results.append(None)

    return results
```

---

## Troubleshooting

### Common Issues

#### 1. Services Won't Start

**Problem**: Containers fail to start

**Solution**:
```bash
# Check logs
docker compose logs -f

# Check if ports are available
netstat -an | grep 4200
netstat -an | grep 5432

# Restart from scratch
docker compose down -v
docker compose up -d
```

#### 2. Worker Not Connecting

**Problem**: Worker can't connect to server

**Solution**:
```bash
# Verify network connectivity
docker compose exec prefect-worker curl http://prefect-server:4200/api/health

# Check environment variables
docker compose exec prefect-worker env | grep PREFECT

# Restart worker
docker compose restart prefect-worker
```

#### 3. Flows Not Executing

**Problem**: Scheduled flows don't run

**Checklist**:
- [ ] Is the worker running? `docker compose ps`
- [ ] Is the deployment active? Check UI at http://localhost:4200
- [ ] Is the work pool correct? `docker compose exec prefect-server prefect work-pool ls`
- [ ] Check worker logs: `docker compose logs -f prefect-worker`

#### 4. Permission Errors

**Problem**: Can't write to outputs directory

**Solution**:
```bash
# Fix permissions
chmod -R 777 outputs/

# Or from container
docker compose exec prefect-worker chmod -R 777 /app/outputs
```

#### 5. Database Issues

**Problem**: PostgreSQL won't start or data is corrupted

**Solution**:
```bash
# Stop services
docker compose down

# Backup current data (if possible)
cp -r data/postgres data/postgres.backup

# Remove corrupted data
rm -rf data/postgres

# Restart (new database will be created)
docker compose up -d
```

#### 6. Out of Memory

**Problem**: Services crashing due to memory

**Solution**:

Edit `docker-compose.yml` and reduce memory limits:
```yaml
deploy:
  resources:
    limits:
      memory: 2G  # Reduce from 5G
```

Then restart:
```bash
docker compose up -d
```

#### 7. Logs Not Appearing in UI

**Problem**: Task logs don't show in Prefect UI

**Solution**:

Ensure you're using `get_run_logger()`:
```python
from prefect.logging import get_run_logger

@task
def my_task():
    logger = get_run_logger()  # â† Use this
    logger.info("Message")      # Not print()
```

---

## Contributing

### Development Setup

1. **Fork the repository**

2. **Create a feature branch**
   ```bash
   git checkout -b feature/my-new-feature
   ```

3. **Make your changes**

4. **Test your changes**
   ```bash
   docker compose build
   docker compose up -d
   # Test your workflows
   ```

5. **Commit and push**
   ```bash
   git add .
   git commit -m "Add new feature"
   git push origin feature/my-new-feature
   ```

6. **Create a Pull Request**

### Code Style

- Follow PEP 8 for Python code
- Use type hints where appropriate
- Add docstrings to all functions
- Include logging statements
- Write descriptive commit messages

### Testing

Test your workflows before deploying:

```bash
# Run locally first
docker compose exec prefect-worker python scripts/my_workflow.py

# Check logs
docker compose logs -f prefect-worker

# Verify in UI
# Open http://localhost:4200
```

---

## Documentation

### Available Guides

- **[README.md](README.md)** - This file (main documentation)
- **[deploy.md](deploy.md)** - Detailed deployment instructions
- **[prefect-user-guide-en.md](prefect-user-guide-en.md)** - Complete user guide (English)
- **[prefect-user-guide-sp.md](prefect-user-guide-sp.md)** - Complete user guide (Spanish)

### External Resources

- **Prefect Documentation**: https://docs.prefect.io
- **Prefect API Reference**: https://docs.prefect.io/api-ref/
- **Prefect Recipes**: https://docs.prefect.io/recipes/
- **Prefect Community**: https://prefect.io/slack
- **Docker Compose Docs**: https://docs.docker.com/compose/

---

## Quick Reference

### Essential Commands

```bash
# Start services
docker compose up -d

# Stop services
docker compose down

# Rebuild worker
docker compose build prefect-worker

# View logs
docker compose logs -f prefect-worker

# Check status
docker compose ps

# Create work pool
docker compose exec prefect-server prefect work-pool create local-pool --type process

# Deploy workflow
docker compose exec prefect-worker python scripts/my_workflow.py

# List deployments
docker compose exec prefect-server prefect deployment ls

# List flows
docker compose exec prefect-server prefect flow ls

# View flow runs
docker compose exec prefect-server prefect flow-run ls

# Access container shell
docker compose exec prefect-worker bash

# Clean everything (CAUTION!)
docker compose down -v
docker system prune -a --volumes
```

### Directory Quick Access

```bash
# View outputs
ls -lh outputs/

# View logs
tail -f logs/worker/*.log

# View data
ls -lh data/postgres/

# Check disk usage
du -sh data/ logs/ outputs/
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Support

If you encounter any issues or have questions:

1. Check the [Troubleshooting](#troubleshooting) section
2. Review the [User Guide](prefect-user-guide-en.md)
3. Check logs: `docker compose logs -f`
4. Consult [Prefect Documentation](https://docs.prefect.io)
5. Open an issue on GitHub

---

## Acknowledgments

- **Prefect** - Modern workflow orchestration
- **Docker** - Containerization platform
- **PostgreSQL** - Reliable database system
- **Redis** - Fast message broker

---

*Last Updated: 2026-01-04*
